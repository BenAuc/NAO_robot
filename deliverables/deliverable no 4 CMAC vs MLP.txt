CMAC vs MPL on the regression task with the Nao robot

MPL showed a better performance compared to the CMAC. We couldn't make a thorough assessment of the CMAC so it is possible the difference is not as large as we think it is. The CMAC yielded a performance during the execution that is less fine-tuned to small changes in the input space. On the other hand the MLP model can make small corrections that follow smoothly the magnitude of the displacements of the tracked object. 

The better performance we observe with the MLP is due to the more complex structure of the MLP (in our final implementation: 2 neurons in first layer, 8 neurons in hidden layer, so 2 x 8 computations and 16 + 8 trainable parameters (weights and biases) for each output, with a linear activation function) in comparision with the 2 x 3/5 (3 or 5 depending on the size of the receptive field) floating point computations / parameters involved in the calculation of each output. Both of our models are in the final form linear models and can therefore compare well. The CMAC however requires more parameters to yield a comparable performance. In operation and under limited computational resources, the CMAC will be faster.

With MSE as loss function and gradient descent our MLP model minimized the error after 5 epochs. The MLP model had a loss of 0,0009998 at this 5th epoch. CMAC on the other hand is a simple, but yet efficent algorithm, which yields good results after 25 epochs of training though, each model relying on 150n data points in each case. The training phase was relatively short for both models, which can be attributed to the simple data set with two inputs and outputs each and the fact that the regression is linear. It is said though (reference section 3.5 in Smith 1998, the thesis) that the CMAC can be trianed more quickly and with a higher learning rate and less iterations than the MLP, for the latter updates all parameters at each iteration (see global generalization below).

The CMAC definitely has as advantage the simplicity of its architecture and training scheme. In comparison with this the MLP must implement a more sophisticated chain rule to train each parameter. Besides the CMAC requires fewer computations to output a response. In the thesis we were meant to read, it is shown that the CMAC is very sensitive to higher variance in the training set i.e. when traininig points are not evenly distributed. Accurate interpolation is highly dependent on the density i.e. non-spasity of the training set. The CMAC however has a predefinite input space (i.e. due to the quantization) beyond which there is no corresponding weights, thus limited by design, which is not the case of the MLP. This means the latter can best handle outliers. Quantization and resolution (in our case a resolution of 50 for input space ranging from 0 to 240 / 320) is also responsible for a more discontinuous mapping to the output space.

Another important different is how CMAC involves local generalization and the MLP global generalization. During training this means that all weights of the MLP are adjusted (hence more computationally costly to train at each epoch) whereas during execution all contributed to the computation of the output. This is not the case for the CMAC as only the activated receptive field is involved in each case. Its local generalization has as benefit that parameter updates do not degrade performance in other areas of the input space. CMACs are likely to overfit to training data and fail to generalize to unseen data and thhefore fail to give a good approximation of the underlying function and to interpolate when training samples are far apart.
